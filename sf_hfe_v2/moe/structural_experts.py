"""
Structural Experts - Data Anatomy Layer
Implements 4 core experts for understanding data structure, organization, and format
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from typing import Dict, List, Any, Optional, Tuple
import logging
import time
from collections import defaultdict, Counter
import re
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

from .base_expert import BaseExpert


class SchemaMapperExpert(BaseExpert):
"""
Expert 1: Schema Mapper Expert
Detects dataset shape, hierarchies, joins, and structural patterns
"""

def __init__(self, expert_id: int = 1, config: Dict[str, Any] = None):
super().__init__(
expert_id=expert_id,
name="SchemaMapperExpert",
relation_types=["structural"],
config=config
)

self.preferred_domains = ["tabular", "relational", "structured"]
self.preferred_data_types = ["float64", "int64", "object"]
self.preferred_tasks = ["schema_detection", "structure_analysis"]

# Schema detection parameters
self.min_samples_for_pattern = self.config.get('min_samples_for_pattern', 10)
self.hierarchy_threshold = self.config.get('hierarchy_threshold', 0.7)

# Pattern storage
self.schema_patterns = defaultdict(list)
self.hierarchy_patterns = defaultdict(list)

def _initialize_expert(self):
"""Initialize schema mapping components"""
self.schema_detector = SchemaDetector()
self.hierarchy_detector = HierarchyDetector()
self.join_detector = JoinDetector()

def process_data(self, data: np.ndarray, metadata: Dict[str, Any] = None) -> Dict[str, Any]:
"""Process data to detect schema and structural patterns"""
metadata = metadata or {}

# Detect basic schema
schema_info = self.schema_detector.detect_schema(data, metadata)

# Detect hierarchies
hierarchy_info = self.hierarchy_detector.detect_hierarchies(data, metadata)

# Detect potential joins
join_info = self.join_detector.detect_joins(data, metadata)

# Generate actual insights
insights = self._generate_structural_insights(schema_info, hierarchy_info, join_info, data)

# Compute confidence based on pattern consistency
confidence = self._compute_schema_confidence(schema_info, hierarchy_info, join_info)

result = {
'expert_id': self.expert_id,
'expert_name': self.name,
'schema_info': schema_info,
'hierarchy_info': hierarchy_info,
'join_info': join_info,
'insights': insights,
'confidence': confidence,
'relation_types': self.relation_types,
'timestamp': time.time()
}

return result

def update_online(self, data: np.ndarray, feedback: Dict[str, Any] = None) -> bool:
"""Update schema patterns based on new data"""
try:
# Extract schema features
schema_features = self.schema_detector.extract_features(data)

# Update pattern memory
self.schema_patterns['recent'].append(schema_features)

# Update hierarchy patterns
hierarchy_features = self.hierarchy_detector.extract_features(data)
self.hierarchy_patterns['recent'].append(hierarchy_features)

# Adapt detection thresholds based on feedback
if feedback and 'accuracy' in feedback:
accuracy = feedback['accuracy']
if accuracy > 0.8:
self.hierarchy_threshold *= 1.01 # Increase sensitivity
elif accuracy < 0.6:
self.hierarchy_threshold *= 0.99 # Decrease sensitivity

self.hierarchy_threshold = max(0.1, min(0.9, self.hierarchy_threshold))

# Store in memory
self.store_memory(data, {'schema_features': schema_features}, feedback)

return True

except Exception as e:
self.logger.error(f"Error updating SchemaMapperExpert: {e}")
return False

def _compute_schema_confidence(self, schema_info: Dict, hierarchy_info: Dict, join_info: Dict) -> float:
"""Compute confidence in schema detection"""
confidence = 0.5 # Base confidence

# Schema consistency
if schema_info.get('is_consistent', False):
confidence += 0.2

# Hierarchy clarity
if hierarchy_info.get('hierarchy_score', 0) > 0.7:
confidence += 0.2

# Join pattern strength
if join_info.get('join_strength', 0) > 0.5:
confidence += 0.1

return min(1.0, confidence)

def _generate_structural_insights(self, schema_info: Dict, hierarchy_info: Dict, join_info: Dict, data: np.ndarray) -> List[str]:
"""Generate actual structural insights from the data"""
insights = []

# Schema insights
if schema_info.get('is_consistent', False):
insights.append(f" Data structure is consistent with {schema_info['row_count']} rows and {schema_info['column_count']} columns")
else:
insights.append(f"âš ï¸ Data structure shows inconsistencies - may need cleaning")

# Hierarchy insights
if hierarchy_info.get('has_hierarchy', False):
hierarchy_score = hierarchy_info.get('hierarchy_score', 0)
insights.append(f"ðŸ”— Hierarchical structure detected (strength: {hierarchy_score:.2f}) with {hierarchy_info.get('levels', 0)} levels")
else:
insights.append(" Flat data structure - no hierarchical relationships detected")

# Join insights
if join_info.get('has_joins', False):
join_strength = join_info.get('join_strength', 0)
insights.append(f"ðŸ”— Potential join relationships detected (strength: {join_strength:.2f})")
else:
insights.append(" No obvious join relationships between columns")

# Data type insights
if data.ndim > 1:
insights.append(f" Multi-dimensional data with {data.shape[1]} features")
if data.shape[1] > 10:
insights.append(" High-dimensional dataset - consider dimensionality reduction")
elif data.shape[1] < 5:
insights.append(" Low-dimensional dataset - suitable for simple analysis")
else:
insights.append(" Single-dimensional data - time series or simple analysis")

# Size insights
if data.size > 10000:
insights.append("ðŸ’¾ Large dataset - consider sampling for initial analysis")
elif data.size < 100:
insights.append("ðŸ“ Small dataset - limited statistical power")
else:
insights.append(" Medium-sized dataset - good for comprehensive analysis")

return insights


class TypeFormatExpert(BaseExpert):
"""
Expert 2: Type & Format Expert
Identifies categorical, numerical, text, timestamp, and other data types
"""

def __init__(self, expert_id: int = 2, config: Dict[str, Any] = None):
super().__init__(
expert_id=expert_id,
name="TypeFormatExpert",
relation_types=["structural"],
config=config
)

self.preferred_domains = ["mixed", "heterogeneous", "tabular"]
self.preferred_data_types = ["object", "mixed"]
self.preferred_tasks = ["type_detection", "format_analysis"]

# Type detection parameters
self.categorical_threshold = self.config.get('categorical_threshold', 0.1)
self.text_threshold = self.config.get('text_threshold', 0.5)
self.timestamp_patterns = [
r'\d{4}-\d{2}-\d{2}', # YYYY-MM-DD
r'\d{2}/\d{2}/\d{4}', # MM/DD/YYYY
r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', # YYYY-MM-DD HH:MM:SS
]

# Type patterns
self.type_patterns = defaultdict(list)

def _initialize_expert(self):
"""Initialize type detection components"""
self.type_detector = TypeDetector()
self.format_analyzer = FormatAnalyzer()
self.encoding_detector = EncodingDetector()

def process_data(self, data: np.ndarray, metadata: Dict[str, Any] = None) -> Dict[str, Any]:
"""Process data to identify types and formats"""
metadata = metadata or {}

# Detect data types
type_info = self.type_detector.detect_types(data, metadata)

# Analyze formats
format_info = self.format_analyzer.analyze_formats(data, metadata)

# Detect encodings
encoding_info = self.encoding_detector.detect_encodings(data, metadata)

# Generate actual insights
insights = self._generate_type_insights(type_info, format_info, encoding_info, data)

# Compute confidence
confidence = self._compute_type_confidence(type_info, format_info, encoding_info)

result = {
'expert_id': self.expert_id,
'expert_name': self.name,
'type_info': type_info,
'format_info': format_info,
'encoding_info': encoding_info,
'insights': insights,
'confidence': confidence,
'relation_types': self.relation_types,
'timestamp': time.time()
}

return result

def update_online(self, data: np.ndarray, feedback: Dict[str, Any] = None) -> bool:
"""Update type patterns based on new data"""
try:
# Extract type features
type_features = self.type_detector.extract_features(data)

# Update type patterns
for detected_type, features in type_features.items():
self.type_patterns[detected_type].append(features)

# Adapt thresholds based on feedback
if feedback and 'type_accuracy' in feedback:
accuracy = feedback['type_accuracy']
if accuracy > 0.9:
self.categorical_threshold *= 1.01
elif accuracy < 0.7:
self.categorical_threshold *= 0.99

self.categorical_threshold = max(0.01, min(0.5, self.categorical_threshold))

# Store in memory
self.store_memory(data, {'type_features': type_features}, feedback)

return True

except Exception as e:
self.logger.error(f"Error updating TypeFormatExpert: {e}")
return False

def _compute_type_confidence(self, type_info: Dict, format_info: Dict, encoding_info: Dict) -> float:
"""Compute confidence in type detection"""
confidence = 0.6 # Base confidence for type detection

# Type clarity
type_clarity = type_info.get('clarity_score', 0)
confidence += type_clarity * 0.3

# Format consistency
format_consistency = format_info.get('consistency_score', 0)
confidence += format_consistency * 0.1

return min(1.0, confidence)

def _generate_type_insights(self, type_info: Dict, format_info: Dict, encoding_info: Dict, data: np.ndarray) -> List[str]:
"""Generate actual type and format insights from the data"""
insights = []

# Type insights
detected_types = type_info.get('detected_types', [])
if detected_types:
type_counts = Counter(detected_types)
most_common_type = type_counts.most_common(1)[0]
insights.append(f" Primary data type: {most_common_type[0]} ({most_common_type[1]}/{len(detected_types)} columns)")

if len(type_counts) > 1:
insights.append(f" Mixed data types detected: {', '.join(type_counts.keys())}")
else:
insights.append(" Consistent data types across all columns")

# Format insights
format_types = format_info.get('format_types', [])
if format_types:
format_counts = Counter(format_types)
if 'date' in format_counts:
insights.append(f"ðŸ“… Date format detected in {format_counts['date']} column(s)")
if 'email' in format_counts:
insights.append(f"ðŸ“§ Email format detected in {format_counts['email']} column(s)")
if 'text' in format_counts:
insights.append(f"ðŸ“ Text data detected in {format_counts['text']} column(s)")

# Encoding insights
detected_encodings = encoding_info.get('detected_encodings', [])
if detected_encodings:
insights.append(f"ðŸ”¤ Encoding detected: {', '.join(detected_encodings)}")

# Data quality insights
clarity_score = type_info.get('clarity_score', 0)
if clarity_score > 0.8:
insights.append(" High type clarity - data types are well-defined")
elif clarity_score > 0.6:
insights.append("âš ï¸ Moderate type clarity - some ambiguity in data types")
else:
insights.append(" Low type clarity - data types are ambiguous")

# Format consistency insights
consistency_score = format_info.get('consistency_score', 0)
if consistency_score > 0.9:
insights.append(" High format consistency across columns")
elif consistency_score > 0.7:
insights.append("âš ï¸ Moderate format consistency")
else:
insights.append(" Low format consistency - mixed formats detected")

return insights


class MissingnessNoiseExpert(BaseExpert):
"""
Expert 3: Missingness & Noise Expert
Quantifies gaps, anomalies, and data reliability
"""

def __init__(self, expert_id: int = 3, config: Dict[str, Any] = None):
super().__init__(
expert_id=expert_id,
name="MissingnessNoiseExpert",
relation_types=["structural", "statistical"],
config=config
)

self.preferred_domains = ["real_world", "sensor", "survey", "incomplete"]
self.preferred_data_types = ["float64", "int64", "object"]
self.preferred_tasks = ["missingness_analysis", "noise_detection", "quality_assessment"]

# Detection parameters
self.missing_threshold = self.config.get('missing_threshold', 0.1)
self.noise_threshold = self.config.get('noise_threshold', 0.05)
self.anomaly_threshold = self.config.get('anomaly_threshold', 0.02)

# Pattern storage
self.missing_patterns = defaultdict(list)
self.noise_patterns = defaultdict(list)

def _initialize_expert(self):
"""Initialize missingness and noise detection components"""
self.missing_detector = MissingDetector()
self.noise_detector = NoiseDetector()
self.anomaly_detector = AnomalyDetector()
self.reliability_assessor = ReliabilityAssessor()

def process_data(self, data: np.ndarray, metadata: Dict[str, Any] = None) -> Dict[str, Any]:
"""Process data to detect missingness, noise, and anomalies"""
metadata = metadata or {}

# Detect missing values
missing_info = self.missing_detector.detect_missing(data, metadata)

# Detect noise
noise_info = self.noise_detector.detect_noise(data, metadata)

# Detect anomalies
anomaly_info = self.anomaly_detector.detect_anomalies(data, metadata)

# Assess reliability
reliability_info = self.reliability_assessor.assess_reliability(data, metadata)

# Generate actual insights
insights = self._generate_quality_insights(missing_info, noise_info, anomaly_info, reliability_info, data)

# Compute confidence
confidence = self._compute_quality_confidence(missing_info, noise_info, anomaly_info, reliability_info)

result = {
'expert_id': self.expert_id,
'expert_name': self.name,
'missing_info': missing_info,
'noise_info': noise_info,
'anomaly_info': anomaly_info,
'reliability_info': reliability_info,
'insights': insights,
'confidence': confidence,
'relation_types': self.relation_types,
'timestamp': time.time()
}

return result

def update_online(self, data: np.ndarray, feedback: Dict[str, Any] = None) -> bool:
"""Update missingness and noise patterns"""
try:
# Extract quality features
missing_features = self.missing_detector.extract_features(data)
noise_features = self.noise_detector.extract_features(data)

# Update patterns
self.missing_patterns['recent'].append(missing_features)
self.noise_patterns['recent'].append(noise_features)

# Adapt thresholds based on feedback
if feedback and 'quality_score' in feedback:
quality_score = feedback['quality_score']
if quality_score > 0.8:
self.missing_threshold *= 1.01
self.noise_threshold *= 1.01
elif quality_score < 0.6:
self.missing_threshold *= 0.99
self.noise_threshold *= 0.99

# Keep thresholds in bounds
self.missing_threshold = max(0.01, min(0.5, self.missing_threshold))
self.noise_threshold = max(0.01, min(0.2, self.noise_threshold))

# Store in memory
self.store_memory(data, {
'missing_features': missing_features,
'noise_features': noise_features
}, feedback)

return True

except Exception as e:
self.logger.error(f"Error updating MissingnessNoiseExpert: {e}")
return False

def _compute_quality_confidence(self, missing_info: Dict, noise_info: Dict, 
anomaly_info: Dict, reliability_info: Dict) -> float:
"""Compute confidence in quality assessment"""
confidence = 0.7 # Base confidence

# Missingness clarity
missing_clarity = missing_info.get('clarity_score', 0)
confidence += missing_clarity * 0.1

# Noise detection strength
noise_strength = noise_info.get('detection_strength', 0)
confidence += noise_strength * 0.1

# Reliability score
reliability_score = reliability_info.get('reliability_score', 0)
confidence += reliability_score * 0.1

return min(1.0, confidence)

def _generate_quality_insights(self, missing_info: Dict, noise_info: Dict, anomaly_info: Dict, reliability_info: Dict, data: np.ndarray) -> List[str]:
"""Generate actual quality insights from the data"""
insights = []

# Missing value insights
missing_ratio = missing_info.get('missing_ratio', 0)
missing_count = missing_info.get('missing_count', 0)
if missing_count > 0:
insights.append(f"âš ï¸ Missing values detected: {missing_count} values ({missing_ratio:.1%} of data)")
if missing_ratio > 0.1:
insights.append(" High missing value rate - data imputation recommended")
elif missing_ratio > 0.05:
insights.append("âš ï¸ Moderate missing value rate - consider imputation")
else:
insights.append(" Low missing value rate - acceptable for analysis")
else:
insights.append(" No missing values detected")

# Noise insights
noise_level = noise_info.get('noise_level', 0)
if noise_level > 0.1:
insights.append(f"ðŸ”Š High noise level detected ({noise_level:.3f}) - data may be unreliable")
elif noise_level > 0.01:
insights.append(f"ðŸ”‰ Moderate noise level ({noise_level:.3f}) - some data smoothing may help")
else:
insights.append("ðŸ”‡ Low noise level - clean data")

# Anomaly insights
anomaly_count = anomaly_info.get('anomaly_count', 0)
anomaly_ratio = anomaly_info.get('anomaly_ratio', 0)
if anomaly_count > 0:
insights.append(f"ðŸš¨ Anomalies detected: {anomaly_count} outliers ({anomaly_ratio:.1%} of data)")
severity = anomaly_info.get('anomaly_severity', 'low')
if severity == 'high':
insights.append(" High anomaly severity - investigate data collection process")
elif severity == 'moderate':
insights.append("âš ï¸ Moderate anomaly severity - review outliers")
else:
insights.append(" Low anomaly severity - outliers may be valid")
else:
insights.append(" No anomalies detected")

# Reliability insights
reliability_score = reliability_info.get('reliability_score', 0)
reliability_level = reliability_info.get('reliability_level', 'unknown')
insights.append(f" Data reliability: {reliability_level} ({reliability_score:.2f})")

if reliability_score > 0.8:
insights.append(" High data reliability - suitable for analysis")
elif reliability_score > 0.6:
insights.append("âš ï¸ Moderate data reliability - proceed with caution")
else:
insights.append(" Low data reliability - data cleaning required")

# Overall quality assessment
quality_indicators = reliability_info.get('quality_indicators', [])
if quality_indicators:
insights.append(f" Quality indicators: {', '.join(quality_indicators)}")

return insights


class ScalingEncodingExpert(BaseExpert):
"""
Expert 4: Scaling & Encoding Expert
Converts heterogeneous columns into unified latent embeddings
"""

def __init__(self, expert_id: int = 4, config: Dict[str, Any] = None):
super().__init__(
expert_id=expert_id,
name="ScalingEncodingExpert",
relation_types=["structural"],
config=config
)

self.preferred_domains = ["mixed", "heterogeneous", "preprocessing"]
self.preferred_data_types = ["mixed", "object", "float64"]
self.preferred_tasks = ["scaling", "encoding", "embedding", "preprocessing"]

# Encoding parameters
self.embedding_dim = self.config.get('embedding_dim', 32)
self.scaling_method = self.config.get('scaling_method', 'standard')
self.encoding_method = self.config.get('encoding_method', 'auto')

# Encoders and scalers
self.scalers = {}
self.encoders = {}
self.embedding_models = {}

def _initialize_expert(self):
"""Initialize scaling and encoding components"""
self.scaler_factory = ScalerFactory()
self.encoder_factory = EncoderFactory()
self.embedding_factory = EmbeddingFactory()

def process_data(self, data: np.ndarray, metadata: Dict[str, Any] = None) -> Dict[str, Any]:
"""Process data to create scaled and encoded representations"""
metadata = metadata or {}

# Analyze scaling needs
scaling_info = self.scaler_factory.analyze_scaling_needs(data, metadata)

# Analyze encoding needs
encoding_info = self.encoder_factory.analyze_encoding_needs(data, metadata)

# Create embeddings
embedding_info = self.embedding_factory.create_embeddings(data, metadata)

# Generate actual insights
insights = self._generate_encoding_insights(scaling_info, encoding_info, embedding_info, data)

# Compute confidence
confidence = self._compute_encoding_confidence(scaling_info, encoding_info, embedding_info)

result = {
'expert_id': self.expert_id,
'expert_name': self.name,
'scaling_info': scaling_info,
'encoding_info': encoding_info,
'embedding_info': embedding_info,
'insights': insights,
'confidence': confidence,
'relation_types': self.relation_types,
'timestamp': time.time()
}

return result

def update_online(self, data: np.ndarray, feedback: Dict[str, Any] = None) -> bool:
"""Update scaling and encoding models"""
try:
# Update scalers
scaling_features = self.scaler_factory.extract_features(data)
for feature_name, features in scaling_features.items():
if feature_name not in self.scalers:
self.scalers[feature_name] = self.scaler_factory.create_scaler(features)
else:
self.scalers[feature_name].partial_fit(features)

# Update encoders
encoding_features = self.encoder_factory.extract_features(data)
for feature_name, features in encoding_features.items():
if feature_name not in self.encoders:
self.encoders[feature_name] = self.encoder_factory.create_encoder(features)
else:
self.encoders[feature_name].partial_fit(features)

# Update embedding models
embedding_features = self.embedding_factory.extract_features(data)
for feature_name, features in embedding_features.items():
if feature_name not in self.embedding_models:
self.embedding_models[feature_name] = self.embedding_factory.create_embedding_model(features)
else:
self.embedding_models[feature_name].update(features)

# Store in memory
self.store_memory(data, {
'scaling_features': scaling_features,
'encoding_features': encoding_features,
'embedding_features': embedding_features
}, feedback)

return True

except Exception as e:
self.logger.error(f"Error updating ScalingEncodingExpert: {e}")
return False

def _compute_encoding_confidence(self, scaling_info: Dict, encoding_info: Dict, embedding_info: Dict) -> float:
"""Compute confidence in encoding quality"""
confidence = 0.8 # Base confidence for encoding

# Scaling quality
scaling_quality = scaling_info.get('quality_score', 0)
confidence += scaling_quality * 0.1

# Encoding quality
encoding_quality = encoding_info.get('quality_score', 0)
confidence += encoding_quality * 0.1

return min(1.0, confidence)

def _generate_encoding_insights(self, scaling_info: Dict, encoding_info: Dict, embedding_info: Dict, data: np.ndarray) -> List[str]:
"""Generate actual encoding and scaling insights from the data"""
insights = []

# Scaling insights
needs_scaling = scaling_info.get('needs_scaling', False)
scaling_method = scaling_info.get('scaling_method', 'none')
if needs_scaling:
insights.append(f"âš–ï¸ Scaling required: {scaling_method} scaling recommended")
insights.append(" Data range suggests normalization will improve analysis")
else:
insights.append(" No scaling required - data is already normalized")

# Encoding insights
needs_encoding = encoding_info.get('needs_encoding', False)
encoding_method = encoding_info.get('encoding_method', 'none')
if needs_encoding:
insights.append(f"ðŸ”¤ Encoding required: {encoding_method} encoding recommended")
insights.append("ðŸ“ Categorical data detected - encoding will enable numerical analysis")
else:
insights.append(" No encoding required - data is already numerical")

# Embedding insights
embedding_dim = embedding_info.get('embedding_dim', 0)
embedding_method = embedding_info.get('embedding_method', 'none')
quality_score = embedding_info.get('quality_score', 0)
if embedding_dim > 0:
insights.append(f"ðŸ§  Embedding created: {embedding_dim}D {embedding_method} embedding")
if quality_score > 0.8:
insights.append(" High embedding quality - good representation of data")
elif quality_score > 0.6:
insights.append("âš ï¸ Moderate embedding quality - acceptable representation")
else:
insights.append(" Low embedding quality - consider different embedding method")
else:
insights.append(" No embedding created - data may not need dimensionality reduction")

# Data preprocessing recommendations
if needs_scaling and needs_encoding:
insights.append(" Full preprocessing pipeline recommended: scaling + encoding")
elif needs_scaling:
insights.append(" Scaling pipeline recommended")
elif needs_encoding:
insights.append(" Encoding pipeline recommended")
else:
insights.append(" Data is ready for analysis - no preprocessing needed")

return insights


# Helper classes for structural experts

class SchemaDetector:
"""Detects schema patterns in data"""

def detect_schema(self, data: np.ndarray, metadata: Dict[str, Any]) -> Dict[str, Any]:
"""Detect schema information"""
schema_info = {
'shape': data.shape,
'dtype': str(data.dtype),
'is_consistent': True,
'has_structure': data.ndim > 1,
'column_count': data.shape[1] if data.ndim > 1 else 1,
'row_count': data.shape[0]
}

# Check for consistency
if data.ndim > 1:
schema_info['is_consistent'] = all(data.shape[0] == data.shape[0] for _ in range(data.shape[1]))

return schema_info

def extract_features(self, data: np.ndarray) -> Dict[str, Any]:
"""Extract schema features"""
return {
'shape': data.shape,
'dtype': str(data.dtype),
'ndim': data.ndim,
'size': data.size
}


class HierarchyDetector:
"""Detects hierarchical patterns in data"""

def detect_hierarchies(self, data: np.ndarray, metadata: Dict[str, Any]) -> Dict[str, Any]:
"""Detect hierarchical patterns"""
hierarchy_info = {
'has_hierarchy': False,
'hierarchy_score': 0.0,
'levels': 0
}

# Simple hierarchy detection based on data structure
if data.ndim > 1 and data.shape[1] > 1:
# Check for potential hierarchical relationships
hierarchy_score = self._compute_hierarchy_score(data)
hierarchy_info['hierarchy_score'] = hierarchy_score
hierarchy_info['has_hierarchy'] = hierarchy_score > 0.5
hierarchy_info['levels'] = min(data.shape[1], 3)

return hierarchy_info

def _compute_hierarchy_score(self, data: np.ndarray) -> float:
"""Compute hierarchy score"""
if data.shape[1] < 2:
return 0.0

# Simple correlation-based hierarchy detection
correlations = np.corrcoef(data.T)
correlation_strength = np.abs(correlations).mean()

return min(1.0, correlation_strength)

def extract_features(self, data: np.ndarray) -> Dict[str, Any]:
"""Extract hierarchy features"""
return {
'hierarchy_score': self._compute_hierarchy_score(data) if data.ndim > 1 else 0.0,
'column_count': data.shape[1] if data.ndim > 1 else 1
}


class JoinDetector:
"""Detects potential join patterns"""

def detect_joins(self, data: np.ndarray, metadata: Dict[str, Any]) -> Dict[str, Any]:
"""Detect join patterns"""
join_info = {
'has_joins': False,
'join_strength': 0.0,
'join_candidates': []
}

# Simple join detection based on data patterns
if data.ndim > 1 and data.shape[1] > 1:
join_strength = self._compute_join_strength(data)
join_info['join_strength'] = join_strength
join_info['has_joins'] = join_strength > 0.3

return join_info

def _compute_join_strength(self, data: np.ndarray) -> float:
"""Compute join strength"""
if data.shape[1] < 2:
return 0.0

# Simple join detection based on uniqueness patterns
unique_ratios = []
for i in range(data.shape[1]):
unique_count = len(np.unique(data[:, i]))
unique_ratio = unique_count / data.shape[0]
unique_ratios.append(unique_ratio)

# Higher join strength if columns have different uniqueness patterns
join_strength = np.std(unique_ratios)
return min(1.0, join_strength)


class TypeDetector:
"""Detects data types"""

def detect_types(self, data: np.ndarray, metadata: Dict[str, Any]) -> Dict[str, Any]:
"""Detect data types"""
type_info = {
'detected_types': [],
'clarity_score': 0.0,
'type_distribution': {}
}

if data.ndim == 1:
# Single column
detected_type = self._detect_column_type(data)
type_info['detected_types'] = [detected_type]
type_info['clarity_score'] = 0.8
else:
# Multiple columns
detected_types = []
for i in range(data.shape[1]):
col_type = self._detect_column_type(data[:, i])
detected_types.append(col_type)

type_info['detected_types'] = detected_types
type_info['clarity_score'] = self._compute_type_clarity(detected_types)

return type_info

def _detect_column_type(self, column: np.ndarray) -> str:
"""Detect type of a single column"""
if column.dtype.kind in ['i', 'f']:
return 'numerical'
elif column.dtype.kind == 'O':
# Check if it's categorical or text
unique_ratio = len(np.unique(column)) / len(column)
if unique_ratio < 0.1:
return 'categorical'
else:
return 'text'
else:
return 'unknown'

def _compute_type_clarity(self, types: List[str]) -> float:
"""Compute clarity of type detection"""
if not types:
return 0.0

# Higher clarity if types are consistent
type_counts = Counter(types)
most_common_ratio = max(type_counts.values()) / len(types)
return most_common_ratio

def extract_features(self, data: np.ndarray) -> Dict[str, Any]:
"""Extract type features"""
if data.ndim == 1:
return {'type': self._detect_column_type(data)}
else:
types = [self._detect_column_type(data[:, i]) for i in range(data.shape[1])]
return {'types': types, 'type_distribution': Counter(types)}


class FormatAnalyzer:
"""Analyzes data formats"""

def analyze_formats(self, data: np.ndarray, metadata: Dict[str, Any]) -> Dict[str, Any]:
"""Analyze data formats"""
format_info = {
'format_types': [],
'consistency_score': 0.0,
'format_patterns': {}
}

if data.ndim == 1:
format_type = self._analyze_column_format(data)
format_info['format_types'] = [format_type]
format_info['consistency_score'] = 0.9
else:
format_types = []
for i in range(data.shape[1]):
format_type = self._analyze_column_format(data[:, i])
format_types.append(format_type)

format_info['format_types'] = format_types
format_info['consistency_score'] = self._compute_format_consistency(format_types)

return format_info

def _analyze_column_format(self, column: np.ndarray) -> str:
"""Analyze format of a single column"""
if column.dtype.kind in ['i', 'f']:
return 'numeric'
elif column.dtype.kind == 'O':
# Check for specific formats
sample_values = column[:min(10, len(column))]
if any(re.match(r'\d{4}-\d{2}-\d{2}', str(val)) for val in sample_values):
return 'date'
elif any('@' in str(val) for val in sample_values):
return 'email'
else:
return 'text'
else:
return 'unknown'

def _compute_format_consistency(self, formats: List[str]) -> float:
"""Compute format consistency"""
if not formats:
return 0.0

format_counts = Counter(formats)
most_common_ratio = max(format_counts.values()) / len(formats)
return most_common_ratio


class EncodingDetector:
"""Detects data encodings"""

def detect_encodings(self, data: np.ndarray, metadata: Dict[str, Any]) -> Dict[str, Any]:
"""Detect data encodings"""
encoding_info = {
'detected_encodings': [],
'encoding_confidence': 0.0
}

# Simple encoding detection
if data.dtype.kind == 'O':
encoding_info['detected_encodings'] = ['utf-8'] # Default assumption
encoding_info['encoding_confidence'] = 0.7
else:
encoding_info['detected_encodings'] = ['binary']
encoding_info['encoding_confidence'] = 0.9

return encoding_info


class MissingDetector:
"""Detects missing values"""

def detect_missing(self, data: np.ndarray, metadata: Dict[str, Any]) -> Dict[str, Any]:
"""Detect missing values"""
missing_info = {
'missing_count': 0,
'missing_ratio': 0.0,
'missing_pattern': 'none',
'clarity_score': 0.0
}

if np.issubdtype(data.dtype, np.floating):
missing_count = np.isnan(data).sum()
missing_ratio = missing_count / data.size

missing_info['missing_count'] = missing_count
missing_info['missing_ratio'] = missing_ratio

if missing_ratio > 0.1:
missing_info['missing_pattern'] = 'high'
elif missing_ratio > 0.01:
missing_info['missing_pattern'] = 'moderate'
else:
missing_info['missing_pattern'] = 'low'

missing_info['clarity_score'] = 1.0 - missing_ratio

return missing_info

def extract_features(self, data: np.ndarray) -> Dict[str, Any]:
"""Extract missing value features"""
if np.issubdtype(data.dtype, np.floating):
missing_count = np.isnan(data).sum()
missing_ratio = missing_count / data.size
return {
'missing_count': missing_count,
'missing_ratio': missing_ratio,
'has_missing': missing_count > 0
}
else:
return {
'missing_count': 0,
'missing_ratio': 0.0,
'has_missing': False
}


class NoiseDetector:
"""Detects noise in data"""

def detect_noise(self, data: np.ndarray, metadata: Dict[str, Any]) -> Dict[str, Any]:
"""Detect noise in data"""
noise_info = {
'noise_level': 0.0,
'detection_strength': 0.0,
'noise_pattern': 'none'
}

if data.size > 4 and np.issubdtype(data.dtype, np.floating):
# Simple noise detection using variance
variance = np.var(data)
mean_val = np.mean(data)

if mean_val != 0:
noise_level = variance / (mean_val ** 2)
else:
noise_level = variance

noise_info['noise_level'] = noise_level
noise_info['detection_strength'] = min(1.0, noise_level)

if noise_level > 0.1:
noise_info['noise_pattern'] = 'high'
elif noise_level > 0.01:
noise_info['noise_pattern'] = 'moderate'
else:
noise_info['noise_pattern'] = 'low'

return noise_info

def extract_features(self, data: np.ndarray) -> Dict[str, Any]:
"""Extract noise features"""
if data.size > 4 and np.issubdtype(data.dtype, np.floating):
variance = np.var(data)
mean_val = np.mean(data)
noise_level = variance / (mean_val ** 2) if mean_val != 0 else variance

return {
'variance': variance,
'noise_level': noise_level,
'has_noise': noise_level > 0.01
}
else:
return {
'variance': 0.0,
'noise_level': 0.0,
'has_noise': False
}


class AnomalyDetector:
"""Detects anomalies in data"""

def detect_anomalies(self, data: np.ndarray, metadata: Dict[str, Any]) -> Dict[str, Any]:
"""Detect anomalies in data"""
anomaly_info = {
'anomaly_count': 0,
'anomaly_ratio': 0.0,
'anomaly_severity': 'none'
}

if data.size > 4 and np.issubdtype(data.dtype, np.floating):
# Simple anomaly detection using IQR method
q1, q3 = np.percentile(data.flatten(), [25, 75])
iqr = q3 - q1

if iqr > 0:
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

anomalies = (data.flatten() < lower_bound) | (data.flatten() > upper_bound)
anomaly_count = np.sum(anomalies)
anomaly_ratio = anomaly_count / data.size

anomaly_info['anomaly_count'] = anomaly_count
anomaly_info['anomaly_ratio'] = anomaly_ratio

if anomaly_ratio > 0.05:
anomaly_info['anomaly_severity'] = 'high'
elif anomaly_ratio > 0.01:
anomaly_info['anomaly_severity'] = 'moderate'
else:
anomaly_info['anomaly_severity'] = 'low'

return anomaly_info


class ReliabilityAssessor:
"""Assesses data reliability"""

def assess_reliability(self, data: np.ndarray, metadata: Dict[str, Any]) -> Dict[str, Any]:
"""Assess data reliability"""
reliability_info = {
'reliability_score': 0.0,
'quality_indicators': [],
'reliability_level': 'unknown'
}

# Simple reliability assessment
reliability_score = 1.0

# Check for missing values
if np.issubdtype(data.dtype, np.floating):
missing_ratio = np.isnan(data).sum() / data.size
reliability_score -= missing_ratio * 0.5

# Check for constant values
if data.size > 1:
if np.all(data == data[0]):
reliability_score -= 0.3

reliability_info['reliability_score'] = max(0.0, reliability_score)

if reliability_score > 0.8:
reliability_info['reliability_level'] = 'high'
elif reliability_score > 0.6:
reliability_info['reliability_level'] = 'moderate'
else:
reliability_info['reliability_level'] = 'low'

return reliability_info


class ScalerFactory:
"""Factory for creating scalers"""

def analyze_scaling_needs(self, data: np.ndarray, metadata: Dict[str, Any]) -> Dict[str, Any]:
"""Analyze scaling needs"""
scaling_info = {
'needs_scaling': False,
'scaling_method': 'none',
'quality_score': 0.0
}

if np.issubdtype(data.dtype, np.floating) and data.size > 1:
# Check if scaling is needed
data_range = np.max(data) - np.min(data)
if data_range > 100: # Arbitrary threshold
scaling_info['needs_scaling'] = True
scaling_info['scaling_method'] = 'standard'
scaling_info['quality_score'] = 0.8

return scaling_info

def extract_features(self, data: np.ndarray) -> Dict[str, Any]:
"""Extract scaling features"""
if np.issubdtype(data.dtype, np.floating):
return {
'mean': np.mean(data),
'std': np.std(data),
'min': np.min(data),
'max': np.max(data),
'range': np.max(data) - np.min(data)
}
else:
return {}


class EncoderFactory:
"""Factory for creating encoders"""

def analyze_encoding_needs(self, data: np.ndarray, metadata: Dict[str, Any]) -> Dict[str, Any]:
"""Analyze encoding needs"""
encoding_info = {
'needs_encoding': False,
'encoding_method': 'none',
'quality_score': 0.0
}

if data.dtype.kind == 'O':
encoding_info['needs_encoding'] = True
encoding_info['encoding_method'] = 'label'
encoding_info['quality_score'] = 0.7

return encoding_info

def extract_features(self, data: np.ndarray) -> Dict[str, Any]:
"""Extract encoding features"""
if data.dtype.kind == 'O':
unique_values = np.unique(data)
return {
'unique_count': len(unique_values),
'total_count': len(data),
'unique_ratio': len(unique_values) / len(data)
}
else:
return {}


class EmbeddingFactory:
"""Factory for creating embeddings"""

def create_embeddings(self, data: np.ndarray, metadata: Dict[str, Any]) -> Dict[str, Any]:
"""Create embeddings"""
embedding_info = {
'embedding_dim': 32,
'embedding_method': 'pca',
'quality_score': 0.0
}

if data.size > 10 and data.ndim > 1:
try:
# Simple PCA embedding
pca = PCA(n_components=min(32, data.shape[1]))
pca.fit(data)
embedding_info['quality_score'] = np.sum(pca.explained_variance_ratio_)
except:
embedding_info['quality_score'] = 0.0

return embedding_info

def extract_features(self, data: np.ndarray) -> Dict[str, Any]:
"""Extract embedding features"""
return {
'shape': data.shape,
'size': data.size,
'ndim': data.ndim
}
